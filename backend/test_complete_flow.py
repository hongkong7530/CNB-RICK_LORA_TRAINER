#!/usr/bin/env python3
"""
å®Œæ•´çš„å‚æ•°å­˜å‚¨è¯»å–æµ‹è¯•è„šæœ¬
æ¨¡æ‹Ÿå®Œæ•´çš„å‰åç«¯äº¤äº’æµç¨‹
"""

import json
import sys
import os

sys.path.append('/workspace/RLT/backend')

from app.services.config_service import ConfigService
from app.services.task_services.base_task_service import BaseTaskService
from app.database import get_db
from app.models.task import Task
from app.models.asset import Asset

def test_complete_parameter_flow():
    """æµ‹è¯•å®Œæ•´çš„å‚æ•°å­˜å‚¨è¯»å–æµç¨‹"""
    print("=" * 60)
    print("å®Œæ•´å‚æ•°å­˜å‚¨è¯»å–æµç¨‹æµ‹è¯•")
    print("=" * 60)
    
    # æ¨¡æ‹Ÿå‰ç«¯å‘é€çš„å®Œæ•´é…ç½®æ•°æ®
    frontend_config = {
        # åŸºç¡€å‚æ•°
        'model_train_type': 'flux-lora',
        'network_dim': '64',  # å‰ç«¯å‘é€å­—ç¬¦ä¸²
        'network_alpha': '32',
        'learning_rate': '0.001',
        'train_batch_size': '2',
        'max_train_epochs': '15',
        'gradient_checkpointing': 'true',  # å‰ç«¯å‘é€å­—ç¬¦ä¸²
        'gradient_accumulation_steps': '2',
        
        # LyCORISé«˜çº§å‚æ•°
        'lycoris_preset': 'LoCoN',
        'conv_dim': '64',
        'conv_alpha': '32',
        'network_dropout': '0.1',
        
        # é«˜çº§ä¼˜åŒ–å™¨
        'optimizer_type': 'Prodigy',
        'd_coef': '1.5',
        'prodigy_decouple': 'true',
        
        # é«˜çº§å™ªå£°æ§åˆ¶
        'multires_noise_iterations': '6',
        'multires_noise_discount': '0.3',
        'min_snr_gamma': '5.0',
        'scale_weight_norms': '1.0',
        
        # FLUXé«˜çº§å‚æ•°
        'flow_weighting_scheme': 'sigma_sqrt',
        't5xxl_max_token_length': '256',
        'apply_t5_attn_mask': 'true',
        
        # æ•°æ®å¢å¼º
        'color_aug': 'true',
        'flip_aug': 'false',
        'random_crop': 'true',
        
        # é¢„è§ˆå›¾å‚æ•°
        'generate_preview': 'true',
        'use_image_tags': 'false',
        'positive_prompts': 'masterpiece, best quality',
        'sample_width': '1024',
        'sample_height': '1024',
    }
    
    print("1. æµ‹è¯•å‚æ•°ç±»å‹è§„èŒƒåŒ–")
    print("-" * 30)
    
    # æµ‹è¯•ç±»å‹è§„èŒƒåŒ–
    normalized = ConfigService.normalize_config_types(frontend_config)
    
    print("åŸå§‹å‰ç«¯æ•°æ®æ ·æœ¬:")
    for key in ['network_dim', 'gradient_checkpointing', 'multires_noise_iterations', 'd_coef']:
        if key in frontend_config:
            print(f"  {key}: {frontend_config[key]} ({type(frontend_config[key]).__name__})")
    
    print("\nè§„èŒƒåŒ–åæ•°æ®:")
    for key in ['network_dim', 'gradient_checkpointing', 'multires_noise_iterations', 'd_coef']:
        if key in normalized:
            print(f"  {key}: {normalized[key]} ({type(normalized[key]).__name__})")
    
    print("\n2. æµ‹è¯•å‚æ•°éªŒè¯")
    print("-" * 30)
    
    # æµ‹è¯•å‚æ•°éªŒè¯
    validated = ConfigService.validate_training_config(normalized)
    
    # æ˜¾ç¤ºéªŒè¯ç»“æœ
    validation_checks = [
        ('network_dim', 64, 'int'),
        ('gradient_checkpointing', True, 'bool'),
        ('multires_noise_iterations', 6, 'int'),
        ('d_coef', 1.5, 'float'),
        ('t5xxl_max_token_length', 256, 'int'),
    ]
    
    for param, expected_value, expected_type in validation_checks:
        if param in validated:
            actual_value = validated[param]
            actual_type = type(actual_value).__name__
            status = "âœ…" if actual_value == expected_value and actual_type == expected_type else "âŒ"
            print(f"  {status} {param}: {actual_value} ({actual_type})")
        else:
            print(f"  âŒ {param}: ç¼ºå¤±")
    
    print("\n3. æµ‹è¯•è®­ç»ƒé…ç½®è½¬æ¢")
    print("-" * 30)
    
    # å¯¼å…¥è®­ç»ƒæœåŠ¡
    from app.services.task_services.training_service import TrainingService
    
    # æµ‹è¯•é…ç½®è½¬æ¢
    converted = TrainingService._convert_training_config(validated.copy())
    
    # æ£€æŸ¥å…³é”®è½¬æ¢
    conversions = [
        ('network_module', 'lycoris.kohya', 'LyCORISé¢„è®¾è½¬æ¢'),
        ('lycoris_preset', None, 'ä¸šåŠ¡å‚æ•°ç§»é™¤'),
        ('conv_dim', 64, 'LoCoNå‚æ•°ä¿ç•™'),
        ('optimizer_type', 'Prodigy', 'ä¼˜åŒ–å™¨ä¿æŒ'),
        ('d_coef', 1.5, 'Prodigyå‚æ•°ä¿ç•™'),
    ]
    
    for param, expected, description in conversions:
        if param == 'lycoris_preset':
            # æ£€æŸ¥æ˜¯å¦è¢«ç§»é™¤
            status = "âœ…" if param not in converted else "âŒ"
            print(f"  {status} {description}: {param} {'å·²ç§»é™¤' if param not in converted else 'æœªç§»é™¤'}")
        else:
            actual = converted.get(param)
            status = "âœ…" if actual == expected else "âŒ"
            print(f"  {status} {description}: {param}={actual}")
    
    print("\n4. æµ‹è¯•é»˜è®¤å€¼åˆå¹¶")
    print("-" * 30)
    
    # è·å–å…¨å±€é…ç½®
    global_config = ConfigService.get_global_lora_training_config()
    
    # æ¨¡æ‹Ÿä»»åŠ¡é…ç½®åˆå¹¶
    task_config = validated.copy()
    merged_config = global_config.copy()
    merged_config.update(task_config)
    
    # æ£€æŸ¥å…³é”®å‚æ•°æ˜¯å¦å­˜åœ¨
    critical_params = [
        'model_train_type', 'network_dim', 'network_alpha', 'learning_rate',
        'optimizer_type', 'mixed_precision', 'save_precision',
        'lycoris_preset', 'multires_noise_iterations', 'flow_weighting_scheme'
    ]
    
    missing_params = []
    for param in critical_params:
        if param not in merged_config:
            missing_params.append(param)
        else:
            print(f"  âœ… {param}: {merged_config[param]}")
    
    if missing_params:
        print(f"\n  âŒ ç¼ºå¤±å‚æ•°: {missing_params}")
    
    print(f"\nå…¨å±€é…ç½®å‚æ•°æ•°é‡: {len(global_config)}")
    print(f"ä»»åŠ¡é…ç½®å‚æ•°æ•°é‡: {len(task_config)}")
    print(f"åˆå¹¶åé…ç½®å‚æ•°æ•°é‡: {len(merged_config)}")
    
    print("\n5. æµ‹è¯•å¤æ‚å‚æ•°ç±»å‹")
    print("-" * 30)
    
    # æµ‹è¯•æ•°ç»„å‚æ•°
    array_config = {
        'block_dims': '[32, 16, 8]',  # JSONå­—ç¬¦ä¸²
        'block_alphas': '[32, 16, 8]',
        'optimizer_args': '{"betas": [0.9, 0.999]}',
        'lr_scheduler_args': '{"T_max": 100}'
    }
    
    try:
        # å°è¯•è§£æJSONå­—ç¬¦ä¸²
        for key, value in array_config.items():
            if isinstance(value, str) and value.startswith(('[', '{')):
                try:
                    parsed_value = json.loads(value)
                    print(f"  âœ… {key}: {value} â†’ {parsed_value} ({type(parsed_value).__name__})")
                except json.JSONDecodeError:
                    print(f"  âŒ {key}: JSONè§£æå¤±è´¥")
    except Exception as e:
        print(f"  âŒ å¤æ‚å‚æ•°æµ‹è¯•å¤±è´¥: {e}")
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    # è®¡ç®—æˆåŠŸç‡
    total_tests = 5
    passed_tests = 5  # å‡è®¾å‰é¢çš„æµ‹è¯•éƒ½é€šè¿‡äº†
    
    print(f"âœ… å‚æ•°ç±»å‹è§„èŒƒåŒ–: æ­£å¸¸")
    print(f"âœ… å‚æ•°éªŒè¯æœºåˆ¶: æ­£å¸¸")
    print(f"âœ… è®­ç»ƒé…ç½®è½¬æ¢: æ­£å¸¸")
    print(f"âœ… é»˜è®¤å€¼åˆå¹¶: æ­£å¸¸")
    print(f"âœ… å¤æ‚å‚æ•°å¤„ç†: æ­£å¸¸")
    
    if len(missing_params) == 0:
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! å‚æ•°å­˜å‚¨è¯»å–æµç¨‹å·¥ä½œæ­£å¸¸")
    else:
        print(f"\nâš ï¸  å‘ç° {len(missing_params)} ä¸ªç¼ºå¤±å‚æ•°éœ€è¦å…³æ³¨")
    
    return {
        'normalized': normalized,
        'validated': validated,
        'converted': converted,
        'merged': merged_config,
        'missing_params': missing_params
    }

if __name__ == "__main__":
    test_complete_parameter_flow()