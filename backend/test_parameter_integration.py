#!/usr/bin/env python3
"""
å‰åç«¯å‚æ•°å¯¹æ¥æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯æ–°å¢çš„LORAè®­ç»ƒå‚æ•°æ˜¯å¦èƒ½å¤Ÿæ­£ç¡®ä¼ é€’å’Œå¤„ç†
"""

import json
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__)))

from app.services.config_service import ConfigService
from app.services.task_services.training_service import TrainingService

def test_parameter_normalization():
    """æµ‹è¯•å‚æ•°ç±»å‹è§„èŒƒåŒ–åŠŸèƒ½"""
    print("=" * 50)
    print("æµ‹è¯•å‚æ•°ç±»å‹è§„èŒƒåŒ–åŠŸèƒ½")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿå‰ç«¯å‘é€çš„æ•°æ®ï¼ˆå­—ç¬¦ä¸²å½¢å¼ï¼‰
    frontend_data = {
        'gradient_checkpointing': 'true',
        'gradient_accumulation_steps': '1',
        'network_dim': '32',
        'network_alpha': '32',
        'learning_rate': '0.0001',
        'multires_noise_iterations': '6',
        'multires_noise_discount': '0.3',
        'lycoris_preset': 'LoCoN',
        'conv_dim': '32',
        'network_dropout': '0.1',
        'd_coef': '1.0',
        'flow_weighting_scheme': 'sigma_sqrt',
        't5xxl_max_token_length': '512',
        'apply_t5_attn_mask': 'true'
    }
    
    print("å‰ç«¯è¾“å…¥æ•°æ®:")
    for key, value in frontend_data.items():
        print(f"  {key}: {value} ({type(value).__name__})")
    
    # è¿›è¡Œç±»å‹è§„èŒƒåŒ–
    normalized = ConfigService.normalize_config_types(frontend_data)
    
    print("\nè§„èŒƒåŒ–åæ•°æ®:")
    for key, value in normalized.items():
        print(f"  {key}: {value} ({type(value).__name__})")
    
    # éªŒè¯ç»“æœ
    assert normalized['gradient_checkpointing'] == True
    assert normalized['gradient_accumulation_steps'] == 1
    assert normalized['network_dim'] == 32
    assert normalized['learning_rate'] == 0.0001
    assert normalized['multires_noise_iterations'] == 6
    assert normalized['apply_t5_attn_mask'] == True
    
    print("\nâœ… å‚æ•°ç±»å‹è§„èŒƒåŒ–æµ‹è¯•é€šè¿‡!")

def test_parameter_validation():
    """æµ‹è¯•å‚æ•°éªŒè¯åŠŸèƒ½"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•å‚æ•°éªŒè¯åŠŸèƒ½")
    print("=" * 50)
    
    # æµ‹è¯•æœ‰æ•ˆå‚æ•°
    valid_config = {
        'network_dim': 64,
        'network_alpha': 32,
        'learning_rate': 0.001,
        'network_dropout': 0.1,
        'caption_dropout_rate': 0.05,
        'multires_noise_iterations': 6,
        'd_coef': 1.5
    }
    
    print("æœ‰æ•ˆé…ç½®è¾“å…¥:")
    for key, value in valid_config.items():
        print(f"  {key}: {value}")
    
    validated = ConfigService.validate_training_config(valid_config)
    
    print("\néªŒè¯åé…ç½®:")
    for key, value in validated.items():
        print(f"  {key}: {value}")
    
    # æµ‹è¯•æ— æ•ˆå‚æ•°ï¼ˆè¶…å‡ºèŒƒå›´ï¼‰
    invalid_config = {
        'network_dim': 1000,  # è¶…å‡ºæœ€å¤§å€¼
        'learning_rate': 0.00001,  # å°äºæœ€å°å€¼
        'network_dropout': 1.5,  # è¶…å‡ºèŒƒå›´
        'multires_noise_iterations': -1,  # è´Ÿå€¼
    }
    
    print("\næ— æ•ˆé…ç½®è¾“å…¥:")
    for key, value in invalid_config.items():
        print(f"  {key}: {value}")
    
    validated_invalid = ConfigService.validate_training_config(invalid_config)
    
    print("\néªŒè¯å¹¶ä¿®æ­£å:")
    for key, value in validated_invalid.items():
        print(f"  {key}: {value}")
    
    # éªŒè¯ä¿®æ­£ç»“æœ
    assert validated_invalid['network_dim'] == 512  # åº”è¯¥è¢«é™åˆ¶ä¸ºæœ€å¤§å€¼
    assert validated_invalid['learning_rate'] == 0.0001  # åº”è¯¥è¢«ä¿®æ­£ä¸ºæœ€å°å€¼
    assert validated_invalid['network_dropout'] == 0.9  # åº”è¯¥è¢«é™åˆ¶ä¸ºæœ€å¤§å€¼
    assert validated_invalid['multires_noise_iterations'] == 0  # åº”è¯¥è¢«ä¿®æ­£ä¸ºæœ€å°å€¼
    
    print("\nâœ… å‚æ•°éªŒè¯æµ‹è¯•é€šè¿‡!")

def test_training_config_conversion():
    """æµ‹è¯•è®­ç»ƒé…ç½®è½¬æ¢åŠŸèƒ½"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•è®­ç»ƒé…ç½®è½¬æ¢åŠŸèƒ½")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿå®Œæ•´çš„è®­ç»ƒé…ç½®
    full_config = {
        'model_train_type': 'flux-lora',
        'flux_model_path': '/path/to/flux/model.safetensors',
        'network_module': 'networks.lora_flux',
        'network_dim': 32,
        'network_alpha': 32,
        'learning_rate': 0.0001,
        'lycoris_preset': 'LoCoN',
        'conv_dim': 32,
        'conv_alpha': 32,
        'optimizer_type': 'Prodigy',
        'd_coef': 1.0,
        'multires_noise_iterations': 6,
        'multires_noise_discount': 0.3,
        'flow_weighting_scheme': 'sigma_sqrt',
        't5xxl_max_token_length': 512,
        'apply_t5_attn_mask': True,
        'color_aug': True,
        'flip_aug': False,
        'generate_preview': True,
        'use_image_tags': True,
        'repeat_num': 12
    }
    
    print("å®Œæ•´é…ç½®è¾“å…¥:")
    for key, value in full_config.items():
        print(f"  {key}: {value}")
    
    # è¿›è¡Œé…ç½®è½¬æ¢
    converted = TrainingService._convert_training_config(full_config)
    
    print("\nè½¬æ¢åé…ç½®:")
    for key, value in converted.items():
        print(f"  {key}: {value}")
    
    # éªŒè¯LyCORISé…ç½®è½¬æ¢
    assert converted.get('network_module') == 'lycoris.kohya'
    assert 'lycoris_preset' not in converted  # åº”è¯¥è¢«ç§»é™¤
    assert converted.get('conv_dim') == 32  # åº”è¯¥ä¿ç•™
    
    # éªŒè¯Prodigyä¼˜åŒ–å™¨é…ç½®
    assert converted.get('optimizer_type') == 'Prodigy'
    assert converted.get('d_coef') == 1.0
    
    # éªŒè¯FLUXç‰¹å®šå‚æ•°ä¿ç•™
    assert converted.get('flow_weighting_scheme') == 'sigma_sqrt'
    assert converted.get('t5xxl_max_token_length') == 512
    
    # éªŒè¯æ•°æ®å¢å¼ºå‚æ•°
    assert converted.get('color_aug') == True
    
    # éªŒè¯ä¸šåŠ¡å‚æ•°è¢«ç§»é™¤
    assert 'use_image_tags' not in converted
    assert 'repeat_num' not in converted
    assert 'generate_preview' not in converted
    
    print("\nâœ… è®­ç»ƒé…ç½®è½¬æ¢æµ‹è¯•é€šè¿‡!")

def test_frontend_backend_defaults_consistency():
    """æµ‹è¯•å‰åç«¯é»˜è®¤å€¼ä¸€è‡´æ€§"""
    print("\n" + "=" * 50)
    print("æµ‹è¯•å‰åç«¯é»˜è®¤å€¼ä¸€è‡´æ€§")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿä»config.pyè·å–çš„åç«¯é»˜è®¤å€¼
    from app.config import Config
    backend_defaults = Config.LORA_TRAINING_CONFIG
    
    # å…³é”®å‚æ•°çš„é»˜è®¤å€¼æ£€æŸ¥
    critical_params = {
        'gradient_checkpointing': True,
        'gradient_accumulation_steps': 1,
        'network_dim': 32,
        'network_alpha': 32,
        'mixed_precision': 'bf16',
        'save_precision': 'fp16',
        'optimizer_type': 'AdamW8bit',
        'lr_scheduler': 'cosine_with_restarts'
    }
    
    print("æ£€æŸ¥å…³é”®å‚æ•°é»˜è®¤å€¼:")
    inconsistencies = []
    
    for param, expected_value in critical_params.items():
        backend_value = backend_defaults.get(param)
        if backend_value != expected_value:
            inconsistencies.append((param, expected_value, backend_value))
            print(f"  âŒ {param}: æœŸæœ› {expected_value}, åç«¯ {backend_value}")
        else:
            print(f"  âœ… {param}: {backend_value}")
    
    if inconsistencies:
        print(f"\nâš ï¸  å‘ç° {len(inconsistencies)} ä¸ªé»˜è®¤å€¼ä¸ä¸€è‡´!")
        for param, expected, actual in inconsistencies:
            print(f"    {param}: æœŸæœ› {expected}, å®é™… {actual}")
    else:
        print("\nâœ… å‰åç«¯é»˜è®¤å€¼ä¸€è‡´æ€§æ£€æŸ¥é€šè¿‡!")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å‰åç«¯å‚æ•°å¯¹æ¥æµ‹è¯•")
    print("=" * 60)
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_parameter_normalization()
        test_parameter_validation()
        test_training_config_conversion()
        test_frontend_backend_defaults_consistency()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! å‰åç«¯å‚æ•°å¯¹æ¥æ­£å¸¸å·¥ä½œ")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()