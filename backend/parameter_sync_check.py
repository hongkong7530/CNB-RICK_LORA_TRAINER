#!/usr/bin/env python3
"""
å‚æ•°åŒæ­¥æ£€æŸ¥è„šæœ¬ - æ¯”è¾ƒå‰ç«¯æœŸæœ›ä¸åç«¯é…ç½®çš„å·®å¼‚
"""

import json
import sys
import os
import re

# å‰ç«¯å‚æ•°å®šä¹‰ï¼ˆä»useLoraParams.jsæå–ï¼‰
FRONTEND_PARAMETERS = {
    # åŸºç¡€é…ç½®
    'model_train_type': 'flux-lora',
    'pretrained_model_name_or_path': '',
    'flux_model_path': '',
    'sd_model_path': '',
    'sdxl_model_path': '',
    'vae': '',
    'sd_vae': '',
    'sdxl_vae': '',
    'ae': '',
    'clip_l': '',
    't5xxl': '',
    'v2': False,
    'train_t5xxl': False,
    
    # è®­ç»ƒé…ç½®
    'train_data_dir': './input',
    'output_dir': './output',
    'output_name': 'rlt',
    'resolution': '768,768',
    'enable_bucket': True,
    'min_bucket_reso': 256,
    'max_bucket_reso': 1024,
    'bucket_reso_steps': 64,
    'bucket_no_upscale': True,
    'train_batch_size': 1,
    'max_train_epochs': 10,
    'save_every_n_epochs': 2,
    'save_model_as': 'safetensors',
    'save_precision': 'fp16',
    
    # å­¦ä¹ ç‡ç›¸å…³
    'learning_rate': 0.0001,
    'unet_lr': 0.0001,
    'text_encoder_lr': 0.00001,
    'lr_scheduler': 'cosine_with_restarts',
    'lr_warmup_steps': 0,
    'lr_scheduler_num_cycles': 1,
    
    # ç½‘ç»œé…ç½®
    'network_module': 'networks.lora_flux',
    'network_dim': 32,
    'network_alpha': 32,
    'network_train_unet_only': False,
    'network_train_text_encoder_only': False,
    
    # ä¼˜åŒ–å™¨é…ç½®
    'optimizer_type': 'AdamW8bit',
    'optimizer_args': {},
    'lr_scheduler_args': {},
    
    # é«˜çº§è®­ç»ƒå‚æ•°
    'gradient_checkpointing': True,
    'gradient_accumulation_steps': 1,
    'mixed_precision': 'bf16',
    'full_fp16': False,
    'full_bf16': False,
    'noise_offset': 0,
    'prior_loss_weight': 1,
    'loss_type': 'l2',
    'guidance_scale': 1,
    
    # FLUXä¸“ç”¨å‚æ•°
    'timestep_sampling': 'sigmoid',
    'sigmoid_scale': 1,
    'model_prediction_type': 'raw',
    'discrete_flow_shift': 1,
    'fp8_base': True,
    'sdpa': True,
    'xformers': False,
    
    # å†…å­˜ä¼˜åŒ–
    'lowram': False,
    'cache_latents': True,
    'cache_latents_to_disk': True,
    'cache_text_encoder_outputs': True,
    'cache_text_encoder_outputs_to_disk': True,
    'persistent_data_loader_workers': True,
    
    # æ–‡æœ¬å¤„ç†
    'caption_extension': '.txt',
    'shuffle_caption': False,
    'keep_tokens': 0,
    'max_token_length': 255,
    'clip_skip': 2,
    'seed': 1337,
    
    # æ—¥å¿—é…ç½®
    'log_with': 'tensorboard',
    'logging_dir': './logs',
    
    # é‡å¤è®­ç»ƒ
    'repeat_num': 12,
    
    # é¢„è§ˆå›¾ç”Ÿæˆ
    'generate_preview': True,
    'use_image_tags': True,
    'max_image_tags': 1,
    'positive_prompts': 'masterpiece, best quality, 1girl, solo',
    'negative_prompts': '(worst quality, low quality:1.4), nsfw',
    'sample_width': 768,
    'sample_height': 1024,
    'sample_cfg': 7,
    'sample_steps': 24,
    'sample_seed': 1337,
    'sample_sampler': 'euler',
    'sample_every_n_epochs': 2,
    
    # LyCORISé«˜çº§ç½‘ç»œæ¨¡å—
    'lycoris_preset': None,
    'conv_dim': None,
    'conv_alpha': None,
    'decompose_both': False,
    'train_norm': False,
    'network_dropout': 0.0,
    'rank_dropout': 0.0,
    'module_dropout': 0.0,
    
    # é«˜çº§ä¼˜åŒ–å™¨å‚æ•°
    'd_coef': 1.0,
    'prodigy_beta3': None,
    'prodigy_decouple': True,
    'prodigy_use_bias_correction': True,
    'prodigy_safeguard_warmup': True,
    
    # é«˜çº§å™ªå£°æ§åˆ¶
    'min_snr_gamma': None,
    'scale_weight_norms': None,
    'caption_dropout_rate': 0.0,
    'caption_dropout_every_n_epochs': 0,
    'caption_tag_dropout_rate': 0.0,
    'multires_noise_iterations': 0,
    'multires_noise_discount': 0.0,
    'ip_noise_gamma': None,
    'ip_noise_gamma_random_strength': False,
    
    # FLUXé«˜çº§å‚æ•°
    'flow_weighting_scheme': 'sigma_sqrt',
    'logit_mean': 0.0,
    'logit_std': 1.0,
    'mode_scale': 1.29,
    'apply_t5_attn_mask': True,
    't5xxl_max_token_length': 512,
    
    # å—å­¦ä¹ ç‡æ§åˆ¶
    'block_lr_zero_threshold': None,
    'down_lr_weight': None,
    'mid_lr_weight': None,
    'up_lr_weight': None,
    'block_dims': None,
    'block_alphas': None,
    
    # å†…å­˜ä¼˜åŒ–
    'vae_batch_size': 0,
    'no_half_vae': False,
    'fp8_base_unet': False,
    
    # æ•°æ®å¢å¼º
    'color_aug': False,
    'flip_aug': False,
    'random_crop': False,
    'arb_max_train_resolution': None,
}

def check_parameter_sync():
    """æ£€æŸ¥å‚æ•°åŒæ­¥çŠ¶æ€"""
    print("=" * 60)
    print("RLTå‚æ•°åŒæ­¥æ£€æŸ¥")
    print("=" * 60)
    
    # å¯¼å…¥åç«¯é…ç½®
    sys.path.append('/workspace/RLT/backend')
    try:
        from app.config import Config
        backend_config = Config.LORA_TRAINING_CONFIG
    except ImportError as e:
        print(f"âŒ æ— æ³•å¯¼å…¥åç«¯é…ç½®: {e}")
        return
    
    # æ¯”è¾ƒå‚æ•°
    missing_in_backend = []
    extra_in_backend = []
    type_mismatches = []
    value_mismatches = []
    
    # æ£€æŸ¥å‰ç«¯å‚æ•°æ˜¯å¦åœ¨åç«¯å­˜åœ¨
    for param, frontend_value in FRONTEND_PARAMETERS.items():
        if param not in backend_config:
            missing_in_backend.append(param)
        else:
            backend_value = backend_config[param]
            # æ£€æŸ¥ç±»å‹åŒ¹é…
            if type(frontend_value) != type(backend_value):
                type_mismatches.append((param, type(frontend_value).__name__, type(backend_value).__name__))
            # æ£€æŸ¥é»˜è®¤å€¼åŒ¹é…ï¼ˆå¿½ç•¥Noneå’Œç©ºå­—ç¬¦ä¸²çš„å·®å¼‚ï¼‰
            elif frontend_value != backend_value and not (frontend_value in [None, ''] and backend_value in [None, '']):
                value_mismatches.append((param, frontend_value, backend_value))
    
    # æ£€æŸ¥åç«¯æ˜¯å¦æœ‰å‰ç«¯æ²¡æœ‰çš„å‚æ•°
    for param in backend_config:
        if param not in FRONTEND_PARAMETERS:
            extra_in_backend.append(param)
    
    # è¾“å‡ºç»“æœ
    print(f"å‰ç«¯å‚æ•°æ€»æ•°: {len(FRONTEND_PARAMETERS)}")
    print(f"åç«¯å‚æ•°æ€»æ•°: {len(backend_config)}")
    print()
    
    if missing_in_backend:
        print(f"âŒ åç«¯ç¼ºå¤±å‚æ•° ({len(missing_in_backend)}ä¸ª):")
        for param in missing_in_backend:
            value = FRONTEND_PARAMETERS[param]
            print(f"  '{param}': {repr(value)},")
        print()
    
    if extra_in_backend:
        print(f"âš ï¸  åç«¯é¢å¤–å‚æ•° ({len(extra_in_backend)}ä¸ª):")
        for param in extra_in_backend:
            print(f"  {param}: {backend_config[param]}")
        print()
    
    if type_mismatches:
        print(f"âš ï¸  ç±»å‹ä¸åŒ¹é… ({len(type_mismatches)}ä¸ª):")
        for param, frontend_type, backend_type in type_mismatches:
            print(f"  {param}: å‰ç«¯={frontend_type}, åç«¯={backend_type}")
        print()
    
    if value_mismatches:
        print(f"âš ï¸  é»˜è®¤å€¼ä¸åŒ¹é… ({len(value_mismatches)}ä¸ª):")
        for param, frontend_value, backend_value in value_mismatches:
            print(f"  {param}: å‰ç«¯={repr(frontend_value)}, åç«¯={repr(backend_value)}")
        print()
    
    # æ€»ç»“
    total_issues = len(missing_in_backend) + len(type_mismatches) + len(value_mismatches)
    if total_issues == 0:
        print("âœ… å‰åç«¯å‚æ•°å®Œå…¨åŒæ­¥!")
    else:
        print(f"ğŸ”¥ å‘ç° {total_issues} ä¸ªåŒæ­¥é—®é¢˜éœ€è¦ä¿®å¤")
    
    return {
        'missing_in_backend': missing_in_backend,
        'extra_in_backend': extra_in_backend,
        'type_mismatches': type_mismatches,
        'value_mismatches': value_mismatches
    }

if __name__ == "__main__":
    check_parameter_sync()